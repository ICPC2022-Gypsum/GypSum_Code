{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asttokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "def add(a,b):\n",
    "    return test_m(a) + b\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "atok = asttokens.ASTTokens(code, parse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 39) FunctionDef def add(a,b):\n",
      "    return test_m(a) + b <_ast.FunctionDef object at 0x7fd2b5807da0>\n",
      "(19, 39) Return return test_m(a) + b <_ast.Return object at 0x7fd2b5807e80>\n",
      "(9, 10) arg a <_ast.arg object at 0x7fd2b5807e10>\n",
      "(11, 12) arg b <_ast.arg object at 0x7fd2b5807e48>\n",
      "(26, 39) BinOp test_m(a) + b <_ast.BinOp object at 0x7fd2b5807eb8>\n",
      "(26, 35) Call test_m(a) <_ast.Call object at 0x7fd2b5807ef0>\n",
      "(38, 39) Name b <_ast.Name object at 0x7fd2b5807f98>\n",
      "(26, 32) Name test_m <_ast.Name object at 0x7fd2b5807f28>\n",
      "(33, 34) Name a <_ast.Name object at 0x7fd2b5807f60>\n"
     ]
    }
   ],
   "source": [
    "for node in ast.walk(atok.tree):\n",
    "  if hasattr(node, 'lineno'):\n",
    "    print(atok.get_text_range(node), node.__class__.__name__, atok.get_text(node),node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'asttokens' has no attribute 'walk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-599c22382620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masttokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'asttokens' has no attribute 'walk'"
     ]
    }
   ],
   "source": [
    "next(n for n in asttokens.walk(atok.tree) if isinstance(n, ast.Attribute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the function is  add\n",
      "firstLine of the function is  2\n",
      "LastLine of the function is  3\n",
      "the source lines are \n",
      "def add(a,b):\n",
      "    return test_m(a) + b\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "tree = ast.parse(code)\n",
    "for function in tree.body:\n",
    "    if isinstance(function, ast.FunctionDef):\n",
    "        # Just in case if there are loops in the definition\n",
    "        lastBody = function.body[-1]\n",
    "        while isinstance(lastBody, (ast.For, ast.While, ast.If)):\n",
    "            lastBody = lastBody.Body[-1]\n",
    "        lastLine = lastBody.lineno\n",
    "        print(\"Name of the function is \", function.name)\n",
    "        print(\"firstLine of the function is \", function.lineno)\n",
    "        print(\"LastLine of the function is \", lastLine)\n",
    "        print(\"the source lines are \")\n",
    "        if isinstance(code, str):\n",
    "            code = code.split(\"\\n\")\n",
    "        for i, line in enumerate(code, 1):\n",
    "            if i in range(function.lineno, lastLine+1):\n",
    "                print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asttokens\n",
    "import javalang\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import sys\n",
    "import tokenize\n",
    "\n",
    "\n",
    "def process_source(file_name, save_file):\n",
    "    with open(file_name, 'r', encoding='utf-8') as source:\n",
    "        lines = source.readlines()\n",
    "    with open(save_file, 'w+', encoding='utf-8') as save:\n",
    "        for line in lines:\n",
    "            code = line.strip()\n",
    "            tokens = list(javalang.tokenizer.tokenize(code))\n",
    "            tks = []\n",
    "            for tk in tokens:\n",
    "                if tk.__class__.__name__ == 'String' or tk.__class__.__name__ == 'Character':\n",
    "                    tks.append('STR_')\n",
    "                elif 'Integer' in tk.__class__.__name__ or 'FloatingPoint' in tk.__class__.__name__:\n",
    "                    tks.append('NUM_')\n",
    "                elif tk.__class__.__name__ == 'Boolean':\n",
    "                    tks.append('BOOL_')\n",
    "                else:\n",
    "                    tks.append(tk.value)\n",
    "            save.write(\" \".join(tks) + '\\n')\n",
    "\n",
    "\n",
    "def get_ast(file_name, w):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(w, 'w+', encoding='utf-8') as wf:\n",
    "        ign_cnt = 0\n",
    "        for line in tqdm(lines):\n",
    "            code = line.strip()\n",
    "            tokens = javalang.tokenizer.tokenize(code)\n",
    "            token_list = list(javalang.tokenizer.tokenize(code))\n",
    "            length = len(token_list)\n",
    "            parser = javalang.parser.Parser(tokens)\n",
    "            try:\n",
    "                tree = parser.parse_member_declaration()\n",
    "            except (javalang.parser.JavaSyntaxError, IndexError, StopIteration, TypeError):\n",
    "                print(code)\n",
    "                continue\n",
    "            flatten = []\n",
    "            for path, node in tree:\n",
    "                flatten.append({'path': path, 'node': node})\n",
    "\n",
    "            ign = False\n",
    "            outputs = []\n",
    "            stop = False\n",
    "            for i, Node in enumerate(flatten):\n",
    "                d = collections.OrderedDict()\n",
    "                path = Node['path']\n",
    "                node = Node['node']\n",
    "                children = []\n",
    "                for child in node.children:\n",
    "                    child_path = None\n",
    "                    if isinstance(child, javalang.ast.Node):\n",
    "                        child_path = path + tuple((node,))\n",
    "                        for j in range(i + 1, len(flatten)):\n",
    "                            if child_path == flatten[j]['path'] and child == flatten[j]['node']:\n",
    "                                children.append(j)\n",
    "                    if isinstance(child, list) and child:\n",
    "                        child_path = path + (node, child)\n",
    "                        for j in range(i + 1, len(flatten)):\n",
    "                            if child_path == flatten[j]['path']:\n",
    "                                children.append(j)\n",
    "                d[\"id\"] = i\n",
    "                d[\"type\"] = str(node)\n",
    "                if children:\n",
    "                    d[\"children\"] = children\n",
    "                value = None\n",
    "                if hasattr(node, 'name'):\n",
    "                    value = node.name\n",
    "                elif hasattr(node, 'value'):\n",
    "                    value = node.value\n",
    "                elif hasattr(node, 'position') and node.position:\n",
    "                    for i, token in enumerate(token_list):\n",
    "                        if node.position == token.position:\n",
    "                            pos = i + 1\n",
    "                            value = str(token.value)\n",
    "                            while pos < length and token_list[pos].value == '.':\n",
    "                                value = value + '.' + token_list[pos + 1].value\n",
    "                                pos += 2\n",
    "                            break\n",
    "                elif type(node) is javalang.tree.This \\\n",
    "                        or type(node) is javalang.tree.ExplicitConstructorInvocation:\n",
    "                    value = 'this'\n",
    "                elif type(node) is javalang.tree.BreakStatement:\n",
    "                    value = 'break'\n",
    "                elif type(node) is javalang.tree.ContinueStatement:\n",
    "                    value = 'continue'\n",
    "                elif type(node) is javalang.tree.TypeArgument:\n",
    "                    value = str(node.pattern_type)\n",
    "                elif type(node) is javalang.tree.SuperMethodInvocation \\\n",
    "                        or type(node) is javalang.tree.SuperMemberReference:\n",
    "                    value = 'super.' + str(node.member)\n",
    "                elif type(node) is javalang.tree.Statement \\\n",
    "                        or type(node) is javalang.tree.BlockStatement \\\n",
    "                        or type(node) is javalang.tree.ForControl \\\n",
    "                        or type(node) is javalang.tree.ArrayInitializer \\\n",
    "                        or type(node) is javalang.tree.SwitchStatementCase:\n",
    "                    value = 'None'\n",
    "                elif type(node) is javalang.tree.VoidClassReference:\n",
    "                    value = 'void.class'\n",
    "                elif type(node) is javalang.tree.SuperConstructorInvocation:\n",
    "                    value = 'super'\n",
    "\n",
    "                if value is not None and type(value) is type('str'):\n",
    "                    d['value'] = value\n",
    "                if not children and not value:\n",
    "                    # print('Leaf has no value!')\n",
    "                    print(type(node))\n",
    "                    print(code)\n",
    "                    ign = True\n",
    "                    ign_cnt += 1\n",
    "                    # break\n",
    "                outputs.append(d)\n",
    "            if not ign:\n",
    "                wf.write(json.dumps(outputs))\n",
    "                wf.write('\\n')\n",
    "    print(ign_cnt)\n",
    "\n",
    "\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "\n",
    "NODE_FIX = '1*NODEFIX'  # '1*NODEFIX'\n",
    "\n",
    "\n",
    "def python2tree(line):\n",
    "    atok = asttokens.ASTTokens(line, parse=True)\n",
    "    return atok, atok.tree\n",
    "\n",
    "\n",
    "def traverse_python_tree(atok, root):\n",
    "    iter_children = asttokens.util.iter_children_func(root)\n",
    "    node_json = []\n",
    "    current_global = {}\n",
    "    current_idx, global_idx = 1, 1\n",
    "    for node in asttokens.util.walk(root):\n",
    "        if not next(iter_children(node), None) is None:\n",
    "            child_num = 0\n",
    "            for child in iter_children(node):\n",
    "                child_num += 1\n",
    "            global_idx = global_idx + child_num\n",
    "            current_global[current_idx] = global_idx\n",
    "        current_idx += 1\n",
    "    # print current_global\n",
    "    current_idx = 1\n",
    "    for node in asttokens.util.walk(root):\n",
    "        # print current_idx\n",
    "        # idx_upper = current_idx\n",
    "        new_node = {\"id\": current_idx, \"type\": type(node).__name__, \"children\": [],\n",
    "                    \"value\": atok.get_text(node)}\n",
    "        if new_node[\"type\"] == 'Name':\n",
    "            new_node['type'] = new_node['value']\n",
    "        idx_upper = len(node_json)\n",
    "        if not next(iter_children(node), None) is None:\n",
    "            child_idx = 0\n",
    "            for child in iter_children(node):\n",
    "                child_idx += 1\n",
    "\n",
    "                new_node['children'].append(\n",
    "                    current_global[current_idx] - child_idx + 1)\n",
    "        else:  # leaf node\n",
    "            new_node['children'].append(atok.get_text(node))\n",
    "        node_json.append(new_node)\n",
    "        current_idx += 1\n",
    "\n",
    "    # update_parent\n",
    "    for k, node in enumerate(node_json):\n",
    "        print(node)\n",
    "        children = [c for c in node['children'] if c.startswith(NODE_FIX)]\n",
    "        if len(children):\n",
    "            for c in children:\n",
    "                node_json[c]['parent'] = k\n",
    "\n",
    "    return node_json\n",
    "\n",
    "\n",
    "def process_python(file, des):\n",
    "    atok, tree = python2tree(open(file, 'r', encoding='utf-8').read())\n",
    "    tree_json = traverse_python_tree(atok, tree)\n",
    "    open(des, 'w', encoding='utf-8').write(json.dumps(tree_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "def add(a,b):\n",
    "    a = b**2\n",
    "    return test_m(a) + b\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "atok, tree = python2tree(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_child(root, iters):\n",
    "    child_iter = next(iters(root), None)\n",
    "    if child_iter is None:\n",
    "        return []\n",
    "    \n",
    "    def expand(nested_list):\n",
    "        for item in iters(nested_list):\n",
    "            if isinstance(item, list):\n",
    "                for sub_item in expand(item):\n",
    "                    yield sub_item\n",
    "            elif item:\n",
    "                yield item\n",
    "\n",
    "    return list(expand(child_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_node(node, iter_children):\n",
    "  children = get_child(node,iter_children)\n",
    "  if len(children)>0:\n",
    "      print(f'node: {node.__class__.__name__} {node._fields}')\n",
    "      try:\n",
    "        print(f'{node.name}')\n",
    "      except:\n",
    "        pass\n",
    "      try:\n",
    "        print(f'{node.op._fields}')\n",
    "      except:\n",
    "        pass\n",
    "  else:\n",
    "      print(\n",
    "          f'node: {node.__class__.__name__} {node._fields} {atok.get_text(node)}')\n",
    "  try:\n",
    "    print('attrs')\n",
    "    for i in node._fields:\n",
    "      if i == 'op':\n",
    "        print(node.op)\n",
    "  except:\n",
    "    pass\n",
    "  print(children)\n",
    "  if children is not []:\n",
    "    cnt = 0\n",
    "    # print('have child')\n",
    "    for child in iter_children(node):\n",
    "      # print(f'child {cnt}')\n",
    "      iter_node(child,iter_children)\n",
    "      cnt += 1\n",
    "  else:  # leaf node\n",
    "    # print('no child')      \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: Module ('body',)\n",
      "attrs\n",
      "[<_ast.arguments object at 0x7fcf44a42518>, <_ast.Assign object at 0x7fcf44a42e48>, <_ast.Return object at 0x7fcf44a42160>]\n",
      "node: FunctionDef ('name', 'args', 'body', 'decorator_list', 'returns')\n",
      "add\n",
      "attrs\n",
      "[<_ast.arg object at 0x7fcf44a428d0>, <_ast.arg object at 0x7fcf44a42e10>]\n",
      "node: arguments ('args', 'vararg', 'kwonlyargs', 'kw_defaults', 'kwarg', 'defaults') a,b\n",
      "attrs\n",
      "[]\n",
      "node: arg ('arg', 'annotation') a\n",
      "attrs\n",
      "[]\n",
      "node: arg ('arg', 'annotation') b\n",
      "attrs\n",
      "[]\n",
      "node: Assign ('targets', 'value') a = b**2\n",
      "attrs\n",
      "[]\n",
      "node: Name ('id', 'ctx') a\n",
      "attrs\n",
      "[]\n",
      "node: BinOp ('left', 'op', 'right') b**2\n",
      "attrs\n",
      "<_ast.Pow object at 0x7fd2ca1b9da0>\n",
      "[]\n",
      "node: Name ('id', 'ctx') b\n",
      "attrs\n",
      "[]\n",
      "node: Num ('n',) 2\n",
      "attrs\n",
      "[]\n",
      "node: Return ('value',)\n",
      "attrs\n",
      "[<_ast.Call object at 0x7fcf44a42828>, <_ast.Name object at 0x7fcf44a424a8>]\n",
      "node: BinOp ('left', 'op', 'right')\n",
      "()\n",
      "attrs\n",
      "<_ast.Add object at 0x7fd2ca1b9710>\n",
      "[<_ast.Name object at 0x7fcf44a42f60>, <_ast.Name object at 0x7fcf44a42e80>]\n",
      "node: Call ('func', 'args', 'keywords') test_m(a)\n",
      "attrs\n",
      "[]\n",
      "node: Name ('id', 'ctx') test_m\n",
      "attrs\n",
      "[]\n",
      "node: Name ('id', 'ctx') a\n",
      "attrs\n",
      "[]\n",
      "node: Name ('id', 'ctx') b\n",
      "attrs\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "iter_children = asttokens.util.iter_children_func(tree)\n",
    "\n",
    "iter_node(tree, iter_children)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'type': 'Module', 'children': [2], 'value': '\\ndef add(a,b):\\n    return test_m(a) + b'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-d93638b21bbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraverse_python_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-671f7a1407c4>\u001b[0m in \u001b[0;36mtraverse_python_tree\u001b[0;34m(atok, root)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNODE_FIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-671f7a1407c4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNODE_FIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "traverse_python_tree(atok, tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_ast.Attribute"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_special_name = ['Module', 'Interactive', 'Expression', 'FunctionType', 'FunctionDef', 'AsyncFunctionDef', 'ClassDef', 'Return', 'Delete', 'Assign', 'AugAssign', 'AnnAssign', 'For', 'AsyncFor', 'While', 'If', 'With', 'AsyncWith', 'Match', 'Raise', 'Try', 'Assert', 'Import', 'ImportFrom', 'Global', 'Nonlocal', 'Expr', 'Pass', 'Break', 'Continue', 'BoolOp', 'NamedExpr', 'BinOp', 'UnaryOp', 'Lambda', 'IfExp', 'Dict', 'Set', 'ListComp', 'SetComp', 'DictComp', 'GeneratorExp', 'Await', 'Yield', 'YieldFrom', 'Compare', 'Call',\n",
    "    'FormattedValue', 'JoinedStr', 'Constant', 'Attribute', 'Subscript', 'Starred', 'Name', 'List', 'Tuple', 'Slice', 'Load', 'Store', 'Del', 'And', 'Or', 'Add', 'Sub', 'Mult', 'MatMult', 'Div', 'Mod', 'Pow', 'LShift', 'RShift', 'BitOr', 'BitXor', 'BitAnd', 'FloorDiv', 'Invert', 'Not', 'UAdd', 'USub', 'Eq', 'NotEq', 'Lt', 'LtE', 'Gt', 'GtE', 'Is', 'IsNot', 'In', 'NotIn', 'ExceptHandler', 'MatchValue', 'MatchSingleton', 'MatchSequence', 'MatchMapping', 'MatchClass', 'MatchStar', 'MatchAs', 'MatchOr', 'TypeIgnore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ast.NodeTransformer at 0x7fcf44a42978>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.NodeTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "class NodeVisitor(ast.NodeVisitor):\n",
    "    def visit_Str(self, tree_node):\n",
    "        print('{}'.format(tree_node.s))\n",
    "\n",
    "\n",
    "class NodeTransformer(ast.NodeTransformer):\n",
    "    def visit_Str(self, tree_node):\n",
    "        return ast.Str('String: ' + tree_node.s)\n",
    "\n",
    "\n",
    "tree_node = ast.parse('''\n",
    "fruits = ['grapes', 'mango']\n",
    "name = 'peter'\n",
    "\n",
    "for fruit in fruits:\n",
    "    print('{} likes {}'.format(name, fruit))\n",
    "''')\n",
    "\n",
    "# NodeTransformer().visit(tree_node)\n",
    "NodeVisitor().visit(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ast.Str at 0x7fcf44a42400>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.Str('String: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/microsoft/codebert-base (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcf4690da20>: Failed to establish a new connection: [Errno -2] Name or service not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             )\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7fcf4690da20>: Failed to establish a new connection: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    755\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/microsoft/codebert-base (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcf4690da20>: Failed to establish a new connection: [Errno -2] Name or service not known',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-a392fba31bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'microsoft/codebert-base'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtokenize_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_<SplitNode>_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m python_special_tokens = ['arguments', 'Module', 'Interactive', 'Expression', 'FunctionType', 'FunctionDef', 'AsyncFunctionDef', 'ClassDef', 'Return', 'Delete', 'Assign', 'AugAssign', 'AnnAssign', 'For', 'AsyncFor', 'While', 'If', 'With', 'AsyncWith', 'Match', 'Raise', 'Try', 'Assert', 'Import', 'ImportFrom', 'Global', 'Nonlocal', 'Expr', 'Pass', 'Break', 'Continue', 'BoolOp', 'NamedExpr', 'BinOp', 'UnaryOp', 'Lambda', 'IfExp', 'Dict', 'Set', 'ListComp', 'SetComp', 'DictComp', 'GeneratorExp', 'Await', 'Yield', 'YieldFrom', 'Compare', 'Call',\n\u001b[1;32m     38\u001b[0m                          'FormattedValue', 'JoinedStr', 'Constant', 'Attribute', 'Subscript', 'Starred', 'Name', 'List', 'Tuple', 'Slice', 'Load', 'Store', 'Del', 'And', 'Or', 'Add', 'Sub', 'Mult', 'MatMult', 'Div', 'Mod', 'Pow', 'LShift', 'RShift', 'BitOr', 'BitXor', 'BitAnd', 'FloorDiv', 'Invert', 'Not', 'UAdd', 'USub', 'Eq', 'NotEq', 'Lt', 'LtE', 'Gt', 'GtE', 'Is', 'IsNot', 'In', 'NotIn', 'ExceptHandler', 'MatchValue', 'MatchSingleton', 'MatchSequence', 'MatchMapping', 'MatchClass', 'MatchStar', 'MatchAs', 'MatchOr', 'TypeIgnore', tokenize_token]\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m             )\n\u001b[1;32m   1657\u001b[0m             additional_files_names = {\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_fast_tokenizer_file\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   3461\u001b[0m     \u001b[0;31m# Inspect all files from the repo/folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m     all_files = get_list_of_files(\n\u001b[0;32m-> 3463\u001b[0;31m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m     )\n\u001b[1;32m   3465\u001b[0m     \u001b[0mtokenizer_files_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_list_of_files\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     model_info = HfApi(endpoint=HUGGINGFACE_CO_RESOLVE_ENDPOINT).model_info(\n\u001b[0;32m-> 1730\u001b[0;31m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m     )\n\u001b[1;32m   1732\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiblings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, token, timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Bearer {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         )\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wy/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/microsoft/codebert-base (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcf4690da20>: Failed to establish a new connection: [Errno -2] Name or service not known',))"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import asttokens\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, DataCollatorWithPadding\n",
    "from anytree import AnyNode\n",
    "from tqdm import tqdm\n",
    "from treelib import Tree\n",
    "import re\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "'''\n",
    "Child 0\n",
    "Parent 1\n",
    "NextSib 2\n",
    "NextUse 3\n",
    "NextToken 4\n",
    "SplitChild 5\n",
    "SplitParent 6\n",
    "SplitNextSib 7\n",
    "LoopNext\n",
    "'''\n",
    "AST_EDGE = 0\n",
    "# Parent = 1\n",
    "NextSib = 1\n",
    "NextUse = 2\n",
    "NextToken = 3\n",
    "# SplitChild = 4\n",
    "# SplitParent = 5\n",
    "# SplitNextSib = 6\n",
    "LoopNext = 4\n",
    "ControlOut = 5\n",
    "ConditionNext = 6\n",
    "\n",
    "checkpoint = 'microsoft/codebert-base'\n",
    "tokenize_token = '_<SplitNode>_'\n",
    "ast_tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "python_special_tokens = ['arguments', 'Module', 'Interactive', 'Expression', 'FunctionType', 'FunctionDef', 'AsyncFunctionDef', 'ClassDef', 'Return', 'Delete', 'Assign', 'AugAssign', 'AnnAssign', 'For', 'AsyncFor', 'While', 'If', 'With', 'AsyncWith', 'Match', 'Raise', 'Try', 'Assert', 'Import', 'ImportFrom', 'Global', 'Nonlocal', 'Expr', 'Pass', 'Break', 'Continue', 'BoolOp', 'NamedExpr', 'BinOp', 'UnaryOp', 'Lambda', 'IfExp', 'Dict', 'Set', 'ListComp', 'SetComp', 'DictComp', 'GeneratorExp', 'Await', 'Yield', 'YieldFrom', 'Compare', 'Call',\n",
    "                         'FormattedValue', 'JoinedStr', 'Constant', 'Attribute', 'Subscript', 'Starred', 'Name', 'List', 'Tuple', 'Slice', 'Load', 'Store', 'Del', 'And', 'Or', 'Add', 'Sub', 'Mult', 'MatMult', 'Div', 'Mod', 'Pow', 'LShift', 'RShift', 'BitOr', 'BitXor', 'BitAnd', 'FloorDiv', 'Invert', 'Not', 'UAdd', 'USub', 'Eq', 'NotEq', 'Lt', 'LtE', 'Gt', 'GtE', 'Is', 'IsNot', 'In', 'NotIn', 'ExceptHandler', 'MatchValue', 'MatchSingleton', 'MatchSequence', 'MatchMapping', 'MatchClass', 'MatchStar', 'MatchAs', 'MatchOr', 'TypeIgnore', tokenize_token]\n",
    "special_tokens_dict = {'additional_special_tokens': python_special_tokens}\n",
    "num_added_toks = ast_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "\n",
    "def visiulize_tree(any_node):\n",
    "    tree = Tree()\n",
    "\n",
    "    def new_tree(node, parent=None):\n",
    "        if node is None:\n",
    "            return\n",
    "        tree.create_node(node.token, node.id, parent=(\n",
    "            None if not parent else parent.id))\n",
    "        for child in node.children:\n",
    "         #  print(child.token)\n",
    "            new_tree(child, node)\n",
    "\n",
    "    new_tree(any_node)\n",
    "\n",
    "    tree.show()\n",
    "\n",
    "# use javalang to generate ASTs and depth-first traverse to generate ast nodes corpus\n",
    "\n",
    "\n",
    "def has_child(node, iters):\n",
    "    return next(iters(node), None) is not None\n",
    "\n",
    "\n",
    "def get_token(node, atok, iters):\n",
    "    token = 'None'\n",
    "    class_name = node.__class__.__name__\n",
    "    if class_name in python_special_tokens and class_name != 'Name':\n",
    "        token = class_name\n",
    "    elif not has_child(node, iters):\n",
    "        token = atok.get_text(node)\n",
    "    else:\n",
    "        token = node.__class__.__name__\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_child(root, iters):\n",
    "    child_iter = next(iters(root), None)\n",
    "    if child_iter is None:\n",
    "        return []\n",
    "\n",
    "    def expand(nested_list):\n",
    "        for item in iters(nested_list):\n",
    "            if isinstance(item, list):\n",
    "                for sub_item in expand(item):\n",
    "                    yield sub_item\n",
    "            elif item:\n",
    "                yield item\n",
    "\n",
    "    children = list(expand(root))\n",
    "    if 'op' in root._fields:\n",
    "        children.append(ast.Str('Add'))\n",
    "\n",
    "\n",
    "def get_sequence(node, sequence):\n",
    "    token, children = get_token(node), get_child(node)\n",
    "    sequence.append(token)\n",
    "    for child in children:\n",
    "        get_sequence(child, sequence)\n",
    "\n",
    "\n",
    "def parse_program(func):\n",
    "    atok = asttokens.ASTTokens(func, parse=True)\n",
    "    return atok, atok.tree\n",
    "\n",
    "\n",
    "#  generate tree for AST Node\n",
    "def create_tree(atok, iters, root, node, node_list, sub_id_list, leave_list, tokenizer, parent=None):\n",
    "    id = len(node_list)\n",
    "    node_list.append(node)\n",
    "    token, children = get_token(node, atok, iters), get_child(node, iters)\n",
    "\n",
    "    if children == []:\n",
    "        # print('this is a leaf:', token, id)\n",
    "        leave_list.append(id)\n",
    "\n",
    "    # Use roberta.tokenizer to generate subtokens\n",
    "    # If a token can be divided into multiple(>1) subtokens, the first subtoken will be set as the previous node,\n",
    "    # and the other subtokens will be set as its new children\n",
    "    token = token.encode('utf-8', 'ignore').decode(\"utf-8\")\n",
    "    sub_token_list = tokenizer.tokenize(token)\n",
    "\n",
    "    if len(sub_token_list) == 1 and sub_token_list[0] in python_special_tokens:\n",
    "        pass\n",
    "    # TODO convert token into lower\n",
    "    else:\n",
    "        sub_tokens = [tokenizer.tokenize(i.lower()) for i in re.sub(\n",
    "            '([a-z0-9])([A-Z])', r'\\1 \\2', token).split()]\n",
    "        for i in range(len(sub_tokens)):\n",
    "            sub_tokens[i][0] = '' + sub_tokens[i][0]\n",
    "        sub_token_list = reduce(operator.add, sub_tokens)\n",
    "        # print(sub_token_list)\n",
    "\n",
    "    #  # TODO \n",
    "    # if children is None or len(children) == 0:\n",
    "    #     sub_token_list[0] = '' + sub_token_list[0]\n",
    "\n",
    "    if id == 0:\n",
    "        # the root node is one of the tokenizer's special tokens\n",
    "        root.token = sub_token_list[0]\n",
    "        root.data = node\n",
    "        # record the num of nodes for every children of root\n",
    "        root_children_node_num = []\n",
    "        for child in children:\n",
    "            node_num = len(node_list)\n",
    "            create_tree(atok, iters, root, child, node_list, sub_id_list,\n",
    "                        leave_list, tokenizer, parent=root)\n",
    "            root_children_node_num.append(len(node_list) - node_num)\n",
    "        return root_children_node_num\n",
    "    else:\n",
    "        new_token = sub_token_list[0] if len(\n",
    "            sub_token_list) <= 1 else tokenize_token\n",
    "        new_node = AnyNode(\n",
    "            id=id, token=new_token, data=node, parent=parent)\n",
    "\n",
    "        if len(sub_token_list) > 1:\n",
    "            sub_id_list.append(id)\n",
    "            for sub_token in sub_token_list:\n",
    "                id += 1\n",
    "                AnyNode(id=id, token=sub_token, data=node, parent=new_node)\n",
    "                node_list.append(sub_token)\n",
    "                sub_id_list.append(id)\n",
    "\n",
    "        for child in children:\n",
    "            create_tree(atok, iters, root, child, node_list, sub_id_list,\n",
    "                        leave_list, tokenizer, parent=new_node)\n",
    "\n",
    "\n",
    "# traverse the AST tree to get all the nodes and edges\n",
    "def get_node_and_edge(node, node_index_list, tokenizer, src, tgt, edge_attrs, variable_token_list, variable_id_list, token_dicts, token_list, token_ids, parent_next=None):\n",
    "    token = node.token\n",
    "    node_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    assert isinstance(node_id, int)\n",
    "    node_index_list.append(node_id)\n",
    "    # node_index_list.append([vocab_dict.word2id.get(token, UNK)])\n",
    "    # find out all variables\n",
    "    token_dicts[node.id] = node.token\n",
    "    if not node.children and token not in python_special_tokens:\n",
    "        token_list.append(token)\n",
    "        token_ids.append(node.id)\n",
    "\n",
    "    # if token in ['VariableDeclarator', 'MemberReference']:\n",
    "    #     if node.children:  # some chidren are comprised by non-utf8 and will be removed\n",
    "    #         child_token = node.children[0].token\n",
    "    #         if child_token == tokenize_token:\n",
    "    #             # print(node.children[0])\n",
    "    #             child_token += ' - '+node.children[0].data\n",
    "    #             print(child_token)\n",
    "    #         variable_token_list.append(child_token)\n",
    "    #         variable_id_list.append(node.children[0].id)\n",
    "\n",
    "    children = node.children\n",
    "    is_split_node = (node.token == tokenize_token)\n",
    "\n",
    "    for idx, child in enumerate(children[:-1]):\n",
    "        # edge_attr = NextSib if not is_split_node else SplitNextSib\n",
    "        if not is_split_node:\n",
    "            edge_attr = NextSib\n",
    "            src.append(child.id)\n",
    "            tgt.append(children[idx+1].id)\n",
    "            edge_attrs.append(edge_attr)\n",
    "\n",
    "            tgt.append(child.id)\n",
    "            src.append(children[idx+1].id)\n",
    "            edge_attrs.append(edge_attr)\n",
    "\n",
    "        # if node.token == 'SwitchStatement' and child.token == 'SwitchStatementCase' and parent_next:\n",
    "        #     if 'BreakStatement' in [i.token for i in child.children]:\n",
    "        #         src.append(child.id)\n",
    "        #         tgt.append(parent_next.id)\n",
    "        #         edge_attrs.append(ControlOut)\n",
    "\n",
    "        #         tgt.append(child.id)\n",
    "        #         src.append(parent_next.id)\n",
    "        #         edge_attrs.append(ControlOut)\n",
    "\n",
    "    # # Control Flow\n",
    "    # if node.token == 'ForStatement' or node.token == 'WhileStatement':\n",
    "    #     # assert len(children) == 2 or (len(children)==3 and children[-1] == 'outer')\n",
    "    #     if len(children) >= 2:\n",
    "    #         src.append(children[1].id)\n",
    "    #         tgt.append(children[0].id)\n",
    "    #         edge_attrs.append(LoopNext)\n",
    "\n",
    "    #         tgt.append(children[1].id)\n",
    "    #         src.append(children[0].id)\n",
    "    #         edge_attrs.append(LoopNext)\n",
    "\n",
    "    # if node.token == 'IfStatement':\n",
    "    #     assert (len(children) == 2 or len(children) == 3) or len(children) == 0\n",
    "    #     if len(children) == 3:\n",
    "    #         # assert children[0].token == 'BinaryOperation'\n",
    "    #         src.append(children[0].id)\n",
    "    #         tgt.append(children[-1].id)\n",
    "    #         edge_attrs.append(ConditionNext)\n",
    "\n",
    "    #         tgt.append(children[0].id)\n",
    "    #         src.append(children[-1].id)\n",
    "    #         edge_attrs.append(ConditionNext)\n",
    "\n",
    "    for idx, child in enumerate(children):\n",
    "        # parent_type = Parent if not is_split_node else SplitParent\n",
    "        # child_type = Child if not is_split_node else SplitChild\n",
    "        src.append(node.id)\n",
    "        tgt.append(child.id)\n",
    "        edge_attrs.append(AST_EDGE)\n",
    "        src.append(child.id)\n",
    "        tgt.append(node.id)\n",
    "        edge_attrs.append(AST_EDGE)\n",
    "        parent_next = children[idx+1] if (idx+1 < len(children)) else None\n",
    "        get_node_and_edge(child, node_index_list, tokenizer,\n",
    "                          src, tgt, edge_attrs, variable_token_list, variable_id_list, token_dicts, token_list, token_ids, parent_next)\n",
    "\n",
    "\n",
    "# generate pytorch_geometric input format data from ast\n",
    "def get_pyg_data_from_ast(atok, ast, tokenizer=ast_tokenizer):\n",
    "    node_list = []\n",
    "    sub_id_list = []  # record the ids of node that can be divide into multple subtokens\n",
    "    leave_list = []  # record the ids of leave\n",
    "    new_tree = AnyNode(id=0, token=None, data=None)\n",
    "    iter_children = asttokens.util.iter_children_func(ast)\n",
    "\n",
    "    root_children_node_num = create_tree(\n",
    "        atok, iter_children, new_tree, ast, node_list, sub_id_list, leave_list, tokenizer)\n",
    "\n",
    "    # print('root_children_node_num', root_children_node_num)\n",
    "    x = []\n",
    "    edge_src = []\n",
    "    edge_tgt = []\n",
    "    edge_attrs = []\n",
    "    # record variable tokens and ids to add data flow edge in AST graph\n",
    "    variable_token_list = []\n",
    "    variable_id_list = []\n",
    "    token_dicts = {}\n",
    "\n",
    "    token_list, token_ids = [], []\n",
    "\n",
    "    get_node_and_edge(new_tree, x, tokenizer, edge_src, edge_tgt, edge_attrs,\n",
    "                      variable_token_list, variable_id_list, token_dicts, token_list, token_ids)\n",
    "    # print(variable_token_list)\n",
    "    # visiulize_tree(new_tree)\n",
    "    # add data flow edge\n",
    "    variable_dict = {}\n",
    "    for i in range(len(variable_token_list)):\n",
    "        if variable_token_list[i] not in variable_dict:\n",
    "            variable_dict.setdefault(\n",
    "                variable_token_list[i], variable_id_list[i])\n",
    "        else:\n",
    "            edge_src.append(variable_dict.get(variable_token_list[i]))\n",
    "            edge_tgt.append(variable_id_list[i])\n",
    "            edge_attrs.append(NextUse)\n",
    "\n",
    "            edge_tgt.append(variable_dict.get(variable_token_list[i]))\n",
    "            edge_src.append(variable_id_list[i])\n",
    "            edge_attrs.append(NextUse)\n",
    "            variable_dict[variable_token_list[i]] = variable_id_list[i]\n",
    "\n",
    "    for idx, item in enumerate(leave_list[:-1]):\n",
    "        edge_src.append(item)\n",
    "        edge_tgt.append(leave_list[idx+1])\n",
    "        edge_attrs.append(NextToken)\n",
    "\n",
    "        edge_tgt.append(item)\n",
    "        edge_src.append(leave_list[idx+1])\n",
    "        edge_attrs.append(NextToken)\n",
    "\n",
    "    edge_index = [edge_src, edge_tgt]\n",
    "\n",
    "    # TODO \n",
    "    if token_list:\n",
    "        token_list[0] = token_list[0].lstrip('')\n",
    "        token_idx = tokenizer.convert_tokens_to_ids(token_list[0])\n",
    "        x[token_ids[0]] = token_idx\n",
    "\n",
    "    return x, edge_index, edge_attrs, root_children_node_num, token_list, token_ids\n",
    "\n",
    "\n",
    "def get_graph_from_source(code, tokenizer=ast_tokenizer):\n",
    "    atok, ast = parse_program(code)\n",
    "    return get_pyg_data_from_ast(atok, ast, tokenizer)\n",
    "\n",
    "\n",
    "def get_subgraph_node_num(root_children_node_num, divide_node_num, max_subgraph_num):\n",
    "    subgraph_node_num = []\n",
    "    node_sum = 0\n",
    "    real_graph_num = 0\n",
    "    for num in root_children_node_num:\n",
    "        node_sum += num\n",
    "        if node_sum >= divide_node_num:\n",
    "            subgraph_node_num.append(node_sum)\n",
    "            node_sum = 0\n",
    "\n",
    "    subgraph_node_num.append(node_sum)\n",
    "    real_graph_num = len(subgraph_node_num)\n",
    "\n",
    "    if real_graph_num >= max_subgraph_num:\n",
    "        return subgraph_node_num[: max_subgraph_num], max_subgraph_num\n",
    "\n",
    "    # print(len(subgraph_node_num))\n",
    "    # if the last subgraph node num < divide_node_num/2, then put the last subgraph to the second to last subgraph\n",
    "    if subgraph_node_num[-1] < divide_node_num/2:\n",
    "        subgraph_node_num[-2] = subgraph_node_num[-2] + subgraph_node_num[-1]\n",
    "        subgraph_node_num[-1] = 0\n",
    "        real_graph_num -= 1\n",
    "\n",
    "    # zero padding for tensor transforming\n",
    "    for _ in range(real_graph_num, max_subgraph_num):\n",
    "        subgraph_node_num.append(0)\n",
    "\n",
    "    return subgraph_node_num, real_graph_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "def add(a,b):\n",
    "  if a>b:\n",
    "    return a-b\n",
    "  return a+b\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Module\n",
      " FunctionDef\n",
      "     If\n",
      "        Compare\n",
      "           a\n",
      "           b\n",
      "        Return\n",
      "            BinOp\n",
      "                a\n",
      "                b\n",
      "     Return\n",
      "        BinOp\n",
      "            a\n",
      "            b\n",
      "     arguments\n",
      "         a\n",
      "         b\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([48720,\n",
       "  50269,\n",
       "  50265,\n",
       "  102,\n",
       "  741,\n",
       "  1106,\n",
       "  45448,\n",
       "  10,\n",
       "  741,\n",
       "  42555,\n",
       "  50284,\n",
       "  10,\n",
       "  741,\n",
       "  42555,\n",
       "  50284,\n",
       "  10,\n",
       "  741],\n",
       " [[0,\n",
       "   1,\n",
       "   2,\n",
       "   5,\n",
       "   5,\n",
       "   13,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   2,\n",
       "   3,\n",
       "   2,\n",
       "   4,\n",
       "   1,\n",
       "   5,\n",
       "   6,\n",
       "   9,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   6,\n",
       "   7,\n",
       "   6,\n",
       "   8,\n",
       "   5,\n",
       "   9,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   10,\n",
       "   11,\n",
       "   10,\n",
       "   12,\n",
       "   1,\n",
       "   13,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   14,\n",
       "   15,\n",
       "   14,\n",
       "   16,\n",
       "   3,\n",
       "   4,\n",
       "   4,\n",
       "   7,\n",
       "   7,\n",
       "   8,\n",
       "   8,\n",
       "   11,\n",
       "   11,\n",
       "   12,\n",
       "   12,\n",
       "   15,\n",
       "   15,\n",
       "   16],\n",
       "  [1,\n",
       "   0,\n",
       "   5,\n",
       "   2,\n",
       "   13,\n",
       "   5,\n",
       "   2,\n",
       "   1,\n",
       "   4,\n",
       "   3,\n",
       "   3,\n",
       "   2,\n",
       "   4,\n",
       "   2,\n",
       "   5,\n",
       "   1,\n",
       "   9,\n",
       "   6,\n",
       "   6,\n",
       "   5,\n",
       "   8,\n",
       "   7,\n",
       "   7,\n",
       "   6,\n",
       "   8,\n",
       "   6,\n",
       "   9,\n",
       "   5,\n",
       "   10,\n",
       "   9,\n",
       "   12,\n",
       "   11,\n",
       "   11,\n",
       "   10,\n",
       "   12,\n",
       "   10,\n",
       "   13,\n",
       "   1,\n",
       "   14,\n",
       "   13,\n",
       "   16,\n",
       "   15,\n",
       "   15,\n",
       "   14,\n",
       "   16,\n",
       "   14,\n",
       "   4,\n",
       "   3,\n",
       "   7,\n",
       "   4,\n",
       "   8,\n",
       "   7,\n",
       "   11,\n",
       "   8,\n",
       "   12,\n",
       "   11,\n",
       "   15,\n",
       "   12,\n",
       "   16,\n",
       "   15]],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3],\n",
       " [16],\n",
       " ['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b'],\n",
       " [3, 4, 7, 8, 11, 12, 15, 16])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_graph_from_source(code, tokenizer=ast_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module\n",
      " FunctionDef\n",
      "     If\n",
      "        Assign\n",
      "           Attribute\n",
      "              spec\n",
      "           spec\n",
      "        If\n",
      "           Compare\n",
      "              Call\n",
      "                 Dict\n",
      "                 _<SplitNode>_\n",
      "                    attr\n",
      "                    get\n",
      "                 _<SplitNode>_\n",
      "                    '\n",
      "                    __\n",
      "                    __\n",
      "                    dict\n",
      "                    '\n",
      "                 spec\n",
      "              entry\n",
      "           Return\n",
      "               false\n",
      "        UnaryOp\n",
      "            Call\n",
      "                _<SplitNode>_\n",
      "                   instance\n",
      "                   is\n",
      "                spec\n",
      "                type\n",
      "     arguments\n",
      "         _<SplitNode>_\n",
      "            _\n",
      "            type\n",
      "            is\n",
      "         entry\n",
      "         spec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"spec entry is_type isinstance spec type entry getattr spec '__dict__' false spec spec\""
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(get_graph_from_source(code, tokenizer=ast_tokenizer)[-2]).replace('',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def add(a,b):\n",
      "    return test_m(a) + b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "def _must_skip(spec, entry, is_type): \n",
    "  if (not isinstance(spec, type)): \n",
    "    if (entry in getattr(spec, '__dict__', {})): \n",
    "      return False \n",
    "    spec = spec.__class__\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ef2a2fcac29384435d8a863080c277ac996ea1b6921b17dd88fa72336a15a6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('wy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
